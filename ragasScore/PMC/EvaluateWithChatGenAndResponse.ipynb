{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules to Import\n",
    "\n",
    "import pandas as pd\n",
    "from EvaluateChatBot import EvalChatBot\n",
    "from ChatBot import ChatBot\n",
    "\n",
    "from datasets import Dataset, Features, Sequence, Value\n",
    "from ragas import evaluate\n",
    "import ast\n",
    "\n",
    "NUMBER_OF_QUESTION = 50\n",
    "# import time\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_df\n",
    "uploaded_file = \"../../pdfData/Cells and Chemistry of Life.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! NOTE: ONLY USE WHEN YOU WANT TO GENERATE NEW QUESTION AND ANSWER!\n",
    "#list_of_eval_question_answer = EvalChatBot(uploaded_file,NUMBER_OF_QUESTION)\n",
    "# questionList = []\n",
    "# answerList = []\n",
    "# contextList = []\n",
    "\n",
    "# for question_answer in list_of_eval_question_answer:\n",
    "#     questionList.append(question_answer['question'])\n",
    "#     answerList.append(question_answer['answer'])\n",
    "#     contextList.append(question_answer['contexts'])\n",
    "#list_of_eval_question_answer[4]['contexts'] #! look at each question and search google for ground_truth answer!\n",
    "#! ONLY RUN WHEN YOU WANT TO GENERATE TESTSET!\n",
    "# df = pd.DataFrame({'question': questionList, 'answer': answerList, 'contexts': contextList})\n",
    "# df.to_csv(\"./testSet.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./testSet.csv\")\n",
    "#df['ground_truth'] = ground_truthList\n",
    "questionList = df['question']\n",
    "answerList = df['answer']\n",
    "contextList = df['contexts']\n",
    "\n",
    "# print(questionList[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ONLY RUN THIS CELL WHEN YOU WANT TO GENERATE TESTSET\n",
    "# i = 19\n",
    "# with open(\"./doingTestSet.txt\", 'w', encoding='utf8') as file:\n",
    "#     file.writelines(\"Answer this question based on this context only. I want to turn your answer become my ground_truth so I can achive above 90 percents ragas score. Answer in 40 words only\" + \"\\n\")\n",
    "#     file.writelines(\"question: \" + str(questionList[i]) + \"\\n\")\n",
    "#     file.writelines(\"answer : \" + str(answerList[i]) + \"\\n\")\n",
    "#     file.writelines(\"context: \" + str(contextList[i]))\n",
    "# print(\"question: \" + str(questionList[4]))\n",
    "# print(\"context: \" + str(contextList[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two key differences visible under a light microscope are: (a) plant cells have a cell wall and chloroplasts, whereas animal cells do not. (b) Plant cells possess a large central vacuole, unlike animal cells, which have smaller vacuoles.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! after a while i self build my own testset\n",
    "ground_truthList = []\n",
    "with open('./ground_truth.txt', 'r') as file:\n",
    "    a = file.readlines()\n",
    "for groundTruth in a:\n",
    "    ground_truthList.append(groundTruth)\n",
    "ground_truthList[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./testSet.csv\")\n",
    "#df['ground_truth'] = ground_truthList\n",
    "questionList = df['question']\n",
    "answerList = df['answer']\n",
    "contextList = df['contexts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextListConvertedEleToList = [ast.literal_eval(context) for context in contextList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(contextListConvertedEleToList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b957b93e11044e68f14daa8ecd9bb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_precision': 0.9458, 'faithfulness': 0.8012, 'answer_relevancy': 0.8245, 'context_recall': 0.9750}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ensure your contexts are lists of strings\n",
    "data = {\n",
    "    \"question\": questionList[:20], #! replace with question list!\n",
    "    \"answer\": answerList[:20],  #! replace with response from chatbot\n",
    "    \"contexts\": contextListConvertedEleToList[:20],  \n",
    "    \"ground_truth\": ground_truthList[:20] #! replace with response from EvaluateChatbot\n",
    "}\n",
    "\n",
    "# Define the features explicitly to ensure correct data types\n",
    "features = Features({\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"answer\": Value(\"string\"),\n",
    "    \"contexts\": Sequence(Value(\"string\")),  # Ensuring contexts is treated as a sequence of strings\n",
    "    \"ground_truth\": Value(\"string\")\n",
    "})\n",
    "\n",
    "# Convert the dictionary to a Dataset with the specified features\n",
    "dataset = Dataset.from_dict(data, features=features)\n",
    "\n",
    "# Perform the evaluation using the adjusted dataset\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
